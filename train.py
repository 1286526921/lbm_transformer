import numpy as np
import torch
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import os
import json
from lbm_transformer import LBMTrajectoryTransformer, physical_constraints



# ===================== æ ¸å¿ƒä¿®å¤ï¼šè®¾å¤‡ä¸è·¯å¾„ä¼˜åŒ– =====================
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
CHECKPOINT_FILE = "latest_checkpoint.pth"  # ç§»é™¤æ–‡ä»¶å¤¹ï¼Œç›´æ¥ä¿å­˜åˆ°å½“å‰ç›®å½•
TRAIN_LOG_FILE = "train_log.json"
BEST_CHECKPOINT_FILE = "best_checkpoint.pth"

print(f"ä½¿ç”¨è®¾å¤‡ï¼š{device}")


# ===================== æ•°æ®é›†ç±»ï¼ˆé€‚é…æ–°æ•°æ®é›†æ ¼å¼ï¼‰ =====================
class LBMDataset(Dataset):
    def __init__(self, data_path):
        if not os.path.exists(data_path):
            raise FileNotFoundError(f"æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨ï¼š{data_path}")

        data = np.load(data_path, allow_pickle=True)
        self.samples = []
        for item in data:
            # ä»æ•°æ®é›†ä¸­ç›´æ¥è·å–é¢„è®¡ç®—çš„ç‰©ç†é‡
            f_input = item["f_non_eq"]       # éå¹³è¡¡æ€åˆ†å¸ƒï¼ˆè¾“å…¥ï¼‰
            f_eq_target = item["f_eq"]       # å¹³è¡¡æ€åˆ†å¸ƒï¼ˆç›®æ ‡ï¼‰
            rho = item["rho"]                # çœŸå®å¯†åº¦ï¼ˆç›´æ¥ä½¿ç”¨ï¼Œæ— éœ€è®¡ç®—ï¼‰
            u = item["u"]                    # çœŸå®é€Ÿåº¦ï¼ˆç›´æ¥ä½¿ç”¨ï¼Œæ— éœ€è®¡ç®—ï¼‰
            self.samples.append((f_input, f_eq_target, rho, u))

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        f_in, f_eq, rho, u = self.samples[idx]
        return (
            torch.tensor(f_in, dtype=torch.float32),
            torch.tensor(f_eq, dtype=torch.float32),
            torch.tensor(rho, dtype=torch.float32),
            torch.tensor(u, dtype=torch.float32)
        )


# ===================== æ–­ç‚¹ä¿å­˜ï¼šå¢å¼ºç‰ˆï¼ˆåŸå­æ“ä½œï¼‰ =====================
def save_checkpoint(model, optimizer, scheduler, epoch, avg_loss, best_loss):
    try:
        model_state = model.module.state_dict() if hasattr(model, 'module') else model.state_dict()

        checkpoint = {
            'epoch': int(epoch),
            'model_state_dict': model_state,
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict(),
            'avg_loss': float(avg_loss),
            'best_loss': float(best_loss)
        }

        # åŸå­æ“ä½œä¿å­˜ï¼šå…ˆå†™ä¸´æ—¶æ–‡ä»¶ï¼Œå†é‡å‘½å
        temp_checkpoint = CHECKPOINT_FILE + ".tmp"
        torch.save(checkpoint, temp_checkpoint)
        os.replace(temp_checkpoint, CHECKPOINT_FILE)  # åŸå­æ“ä½œæ›¿æ¢

        if avg_loss < best_loss:
            temp_best = BEST_CHECKPOINT_FILE + ".tmp"
            torch.save(checkpoint, temp_best)
            os.replace(temp_best, BEST_CHECKPOINT_FILE)  # åŸå­æ“ä½œæ›¿æ¢
            print(f"âœ… æœ€ä½³æ¨¡å‹å·²å¤‡ä»½ï¼š{BEST_CHECKPOINT_FILE}")

        log_data = {
            'current_epoch': int(epoch),
            'best_loss': float(best_loss),
            'last_loss': float(avg_loss),
            'save_time': str(os.popen('date').read().strip())
        }
        # æ—¥å¿—æ–‡ä»¶ä¹Ÿä½¿ç”¨åŸå­æ“ä½œ
        temp_log = TRAIN_LOG_FILE + ".tmp"
        with open(temp_log, 'w', encoding='utf-8') as f:
            json.dump(log_data, f, indent=4, ensure_ascii=False)
        os.replace(temp_log, TRAIN_LOG_FILE)

        print(f"âœ… æ–­ç‚¹å·²ä¿å­˜ï¼šç¬¬{epoch}è½®ï¼ŒæŸå¤±={avg_loss:.6f}")

    except PermissionError:
        print(f"âŒ æƒé™é”™è¯¯ï¼šæ— æ³•å†™å…¥æ–‡ä»¶ {CHECKPOINT_FILE}")
    except Exception as e:
        print(f"âŒ ä¿å­˜æ–­ç‚¹å¤±è´¥ï¼š{str(e)}")


# ===================== æ–­ç‚¹åŠ è½½ï¼šå¢å¼ºç‰ˆ =====================
def load_checkpoint(model, optimizer, scheduler):
    if not os.path.exists(CHECKPOINT_FILE):
        print("âš ï¸ æœªæ‰¾åˆ°æ–­ç‚¹æ–‡ä»¶ï¼Œå°†ä»å¤´å¼€å§‹è®­ç»ƒ")
        return False, 0, float('inf')

    try:
        checkpoint = torch.load(CHECKPOINT_FILE, map_location=device)
        model.load_state_dict(checkpoint['model_state_dict'], strict=False)

        if optimizer and 'optimizer_state_dict' in checkpoint:
            try:
                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            except Exception as e:
                print(f"âš ï¸ ä¼˜åŒ–å™¨çŠ¶æ€åŠ è½½å¤±è´¥ï¼š{str(e)}")

        if scheduler and 'scheduler_state_dict' in checkpoint:
            try:
                scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
            except Exception as e:
                print(f"âš ï¸ è°ƒåº¦å™¨çŠ¶æ€åŠ è½½å¤±è´¥ï¼š{str(e)}")

        start_epoch = int(checkpoint.get('epoch', 0)) + 1
        best_loss = float(checkpoint.get('best_loss', float('inf')))
        last_loss = float(checkpoint.get('avg_loss', 0.0))

        print(f"âœ… æˆåŠŸåŠ è½½æ–­ç‚¹ï¼šä»ç¬¬{start_epoch}è½®å¼€å§‹è®­ç»ƒï¼ˆä¸Šä¸€è½®æŸå¤±={last_loss:.6f}ï¼Œæœ€ä½³æŸå¤±={best_loss:.6f}ï¼‰")
        return True, start_epoch, best_loss

    except RuntimeError as e:
        print(f"âŒ æ¨¡å‹å‚æ•°ä¸åŒ¹é…ï¼š{str(e)}ï¼Œå°†ä»å¤´å¼€å§‹è®­ç»ƒ")
        return False, 0, float('inf')
    except Exception as e:
        print(f"âŒ æ–­ç‚¹æ–‡ä»¶æŸåï¼š{str(e)}ï¼Œå°†ä»å¤´å¼€å§‹è®­ç»ƒ")
        return False, 0, float('inf')


# ===================== è®­ç»ƒå‡½æ•° =====================
def train():
    # åŸºç¡€é…ç½®ï¼ˆæ›´æ–°æ•°æ®é›†è·¯å¾„ä¸ºlbm_dataset.pyç”Ÿæˆçš„è·¯å¾„ï¼‰
    global device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    batch_size = 32
    total_epochs = 5528
    lr = 1e-4
    data_path = "lbm_dataset_final_no_weight.npy"  # åŒ¹é…lbm_dataset.pyçš„SAVE_PATH

    # æ•°æ®åŠ è½½
    try:
        dataset = LBMDataset(data_path)
        train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
        print(f"âœ… æ•°æ®é›†åŠ è½½æˆåŠŸï¼Œå…± {len(dataset)} ä¸ªæ ·æœ¬")
    except Exception as e:
        print(f"âŒ æ•°æ®é›†åŠ è½½å¤±è´¥ï¼š{str(e)}")
        return

    # æ¨¡å‹åˆå§‹åŒ–
    model = LBMTrajectoryTransformer().to(device)
    if torch.cuda.device_count() > 1:
        model = torch.nn.DataParallel(model)
        print(f"âœ… ä½¿ç”¨ {torch.cuda.device_count()} ä¸ªGPUè®­ç»ƒ")

    optimizer = optim.Adam(model.parameters(), lr=lr)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_epochs)

    # åŠ è½½æ–­ç‚¹
    loaded, start_epoch, best_loss = load_checkpoint(model, optimizer, scheduler)

    # è®­ç»ƒå¾ªç¯
    for epoch in range(start_epoch, total_epochs):
        model.train()
        total_loss = 0.0

        for batch_idx, (f_in, f_eq_target, rho, u) in enumerate(train_loader):
            f_in = f_in.unsqueeze(1).to(device)  # (batch,1,9)
            f_eq_target = f_eq_target.to(device)
            rho = rho.to(device)
            u = u.to(device)

            # å‰å‘ä¼ æ’­
            f_eq_pred = model(f_in)

            e = np.array([[0, 0], [1, 0], [0, 1], [-1, 0], [0, -1],
                          [1, 1], [-1, 1], [-1, -1], [1, -1]],dtype=np.float64)  # (9,2) ç¦»æ•£é€Ÿåº¦æ–¹å‘
            # åœ¨æŸå¤±è®¡ç®—å‰å®šä¹‰D2Q9æƒé‡ï¼ˆä¸lbm_dataset.pyä¿æŒä¸€è‡´ï¼‰
            w = torch.tensor([
                4 / 9, 1 / 9, 1 / 9, 1 / 9, 1 / 9,
                1 / 36, 1 / 36, 1 / 36, 1 / 36
            ], device=device, dtype=torch.float32)  # ç¡®ä¿ä¸æ¨¡å‹åœ¨åŒä¸€è®¾å¤‡

            # æŸå¤±è®¡ç®—ï¼ˆå¢åŠ æƒé‡ï¼‰
            # 1. åŠ æƒMSEæŸå¤±ï¼šå¯¹æ¯ä¸ªé€Ÿåº¦æ–¹å‘çš„è¯¯å·®æŒ‰æƒé‡wåŠ æƒ
            weighted_pred = f_eq_pred * w  # é¢„æµ‹å€¼åŠ æƒ
            weighted_target = f_eq_target * w  # ç›®æ ‡å€¼åŠ æƒ
            mse_loss = F.mse_loss(weighted_pred, weighted_target)

            # 2. ç‰©ç†çº¦æŸæŸå¤±ï¼ˆä¿æŒä¸å˜ï¼Œå·²å†…ç½®æƒé‡ï¼‰
            phys_loss = physical_constraints(f_eq_pred, rho, u, e)

            # 3. æ­£åˆ™æŸå¤±ï¼ˆä¿æŒä¸å˜ï¼‰
            reg_loss = sum(p.pow(2).sum() for p in model.parameters()) * 1e-5

            # æ€»æŸå¤±
            loss = mse_loss + phys_loss + reg_loss

            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        # è®¡ç®—å¹³å‡æŸå¤±
        avg_loss = total_loss / len(train_loader)

        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()

        # æ›´æ–°æœ€ä½³æŸå¤±
        current_best_loss = min(best_loss, avg_loss)

        # ä¿å­˜æ–­ç‚¹
        save_checkpoint(model, optimizer, scheduler, epoch, avg_loss, current_best_loss)
        best_loss = current_best_loss

        # æ‰“å°æ—¥å¿—
        print(
            f"ğŸ“Œ Epoch [{epoch + 1}/{total_epochs}], Loss: {avg_loss:.6f}, Best Loss: {best_loss:.6f}, LR: {scheduler.get_last_lr()[0]:.6e}")

    # è®­ç»ƒå®Œæˆï¼šä¿å­˜æœ€ç»ˆæ¨¡å‹
    try:
        final_model_path = "lbm_transformer_final.pth"
        model_state = model.module.state_dict() if hasattr(model, 'module') else model.state_dict()
        # æœ€ç»ˆæ¨¡å‹ä¹Ÿä½¿ç”¨åŸå­æ“ä½œä¿å­˜
        temp_final = final_model_path + ".tmp"
        torch.save(model_state, temp_final)
        os.replace(temp_final, final_model_path)
        print(f"ğŸ‰ è®­ç»ƒå®Œæˆï¼æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜ä¸º {final_model_path}")
    except Exception as e:
        print(f"âŒ ä¿å­˜æœ€ç»ˆæ¨¡å‹å¤±è´¥ï¼š{str(e)}")


if __name__ == "__main__":
    train()